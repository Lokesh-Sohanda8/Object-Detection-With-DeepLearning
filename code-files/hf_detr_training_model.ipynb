{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c028dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q transformers torch torchvision pillow opencv-python numpy timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298157d",
   "metadata": {},
   "source": [
    "### CLI Command for Running the DETR (DEtection TRansformer) Model by Facebook\n",
    "- Specifically, ResNet-50 in the case of the detr-resnet-50 variant\n",
    "\n",
    "```\n",
    "!yolo predict model=rtdetr-l.pt source='/content/drive/MyDrive/video.mp4' device=0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea2060",
   "metadata": {},
   "source": [
    "### Ran on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bcfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "# Import the Colab-specific patch\n",
    "from google.colab.patches import cv2_imshow\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Use 0 for webcam, or provide the exact path to your video file\n",
    "VIDEO_PATH = '/content/drive/MyDrive/AI_Object_Detection_Project_P14/driver-action-recognition.mp4'\n",
    "OUTPUT_PATH = 'detr_output.avi'\n",
    "\n",
    "# We use the standard DETR model from Facebook (Meta)\n",
    "MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "\n",
    "# --- 2. LOAD HUGGING FACE PIPELINE ---\n",
    "print(f\"[INFO] Downloading/Loading model: {MODEL_NAME}...\")\n",
    "# The 'object-detection' pipeline handles preprocessing, inference, and post-processing\n",
    "detector = pipeline(\"object-detection\", model=MODEL_NAME)\n",
    "print(\"[INFO] Model loaded successfully.\")\n",
    "\n",
    "# --- 3. VIDEO SETUP ---\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"[ERROR] Cannot open video: {VIDEO_PATH}\")\n",
    "else:\n",
    "    # Get video properties for the writer\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) if cap.get(cv2.CAP_PROP_FPS) > 0 else 20\n",
    "\n",
    "    # Use MJPG/AVI for safety on your laptop/Colab\n",
    "    out = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'MJPG'), fps, (frame_width, frame_height))\n",
    "\n",
    "    print(f\"[INFO] Starting inference on {VIDEO_PATH}...\")\n",
    "    print(\"[INFO] Processing... (This may take time)\")\n",
    "\n",
    "    # --- 4. PROCESSING LOOP ---\n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            # CONVERSION STEP: OpenCV uses BGR, but Hugging Face needs RGB PIL Images\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(rgb_frame)\n",
    "\n",
    "            # --- AI INFERENCE ---\n",
    "            results = detector(pil_image)\n",
    "\n",
    "            # --- DRAWING RESULTS ---\n",
    "            for result in results:\n",
    "                box = result['box']\n",
    "                label = result['label']\n",
    "                score = result['score']\n",
    "\n",
    "                # Only draw if confident (e.g., > 50%)\n",
    "                if score > 0.5:\n",
    "                    xmin, ymin = int(box['xmin']), int(box['ymin'])\n",
    "                    xmax, ymax = int(box['xmax']), int(box['ymax'])\n",
    "\n",
    "                    # Draw Box (Green)\n",
    "                    cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "\n",
    "                    # Draw Label\n",
    "                    label_text = f\"{label}: {score:.2f}\"\n",
    "                    cv2.putText(frame, label_text, (xmin, ymin - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Save the annotated frame to output file\n",
    "            out.write(frame)\n",
    "\n",
    "            # --- COLAB DISPLAY LOGIC ---\n",
    "            # To avoid spamming the notebook with images, we clear output and show the latest frame\n",
    "            # Using wait=True makes the transition smoother\n",
    "            clear_output(wait=True)\n",
    "            cv2_imshow(frame)\n",
    "\n",
    "            # Note: cv2.waitKey works with cv2.imshow (local), but not cv2_imshow (colab).\n",
    "            # We keep it for compatibility if you export this script later.\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"[INFO] Interrupted by user.\")\n",
    "\n",
    "    finally:\n",
    "        # --- CLEANUP ---\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"\\n[INFO] Finished! Processed {frame_count} frames in {duration:.2f} seconds.\")\n",
    "        print(f\"[INFO] Average FPS: {frame_count / duration:.2f}\")\n",
    "        print(f\"[INFO] Output saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
